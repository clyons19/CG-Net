{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c26fe9c",
   "metadata": {},
   "source": [
    "CG-Net.ipynb \n",
    "* Created by: Carter Lyons\n",
    "\n",
    "Contains:\n",
    "* Deep neural network implementation of Compound Gaussian Network (CG-Net) form by applying algorithm unrolling to the Compound Gaussian Least Squares (CG-LS) iterative algorithm. Estimates signal representation coefficients from linear measurements.\n",
    "* For more information reference:\n",
    "    1. Lyons C., Raj R. G., and Cheney M. (2023). \"A Compound Gaussian Network for Solving Linear Inverse Problems,\" In-Review.\n",
    "    2. Lyons C., Raj R. G., and Cheney M. (2022). \"CG-Net: A Compound Gaussian Prior Based Unrolled Imaging Network,\" in *2022 IEEE Asia-Pacific Signal and Information Processing Association Annual Summit and Conference*, pp. 623-629.\n",
    "\n",
    "Package Requirements (in addition to those required in Initialize.py):\n",
    "* Tensorflow 2.5.0, pandas, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d104082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary python packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Import necessary additional code files\n",
    "import Initialize\n",
    "import LossFunctions\n",
    "from CGNet import CGNet\n",
    "from CGNet import Grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd2c0a",
   "metadata": {},
   "source": [
    "# Create Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "556a417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose network parameters\n",
    "img_size = 32  # Height and width of input images \n",
    "style = 'tridiag'  # Format of PD matrix in z quadratic descent direction. Options: 'tridiag', 'full'\n",
    "angles = 15  # Number of uniformly spaced angles for Radon transform\n",
    "noise = 60  # SNR of measurements (in dB)\n",
    "num_training_samples = 20  # Number of training samples to be used\n",
    "num_epochs, batch_size, TestBatchSize = 20, 20, 50  # Number of epochs for training, training batch size, and test batch size\n",
    "K = 20  # Number of iterations to unroll\n",
    "J = 1  # Number of steepest descent updates of z for each interation to unroll\n",
    "scale = 1  # Parameter to scale the input measurements by\n",
    "lamb, kappa, eta, eps = 0.3, 2, 0.5, 1e-3  # Initialization of regularization paramters lamb, kappa. Initialization of step size eta. Stabilizing paramter eps.\n",
    "UseValidationData, ValidationSamples = True, 200  # Use validation data during training to check for overfitting. Number of samples to validate the network with.\n",
    "# path_to_save = r'path_to_desired_save_folder'  # Path to save weights trained by the network.\n",
    "path_to_save = r'C:\\Users\\LyonsC\\Desktop\\New folder'\n",
    "\n",
    "n = img_size**2  # Signal size\n",
    "B = Initialize.create_measurements('barbara.png', [[154, 154+img_size],[64,64+img_size]], import_phi = (False, [], 'bior1.1'), import_psi = (False, [], angles))  # From Initialize.py: Create Radon transform matrix. Create biorthogonal wavelet matrix. Use these matrices to form measurements of input 'Barbara' image\n",
    "A = tf.constant(B.Psi@B.Phi, dtype = float)  # Measurement matrix: A = Psi*Phi (set dtype to float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e856a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network structure\n",
    "model = CGNet(K, J, A, style, lamb, kappa = kappa, eta = eta, eps = eps, method = 'g', normalize = True)\n",
    "model = model.call()\n",
    "\n",
    "grad = Grad()\n",
    "loss_fnc = LossFunctions.SSIM_loss(B.Phi, img_size, scale)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4003815",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()  # Display summary of network structure (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b3c08",
   "metadata": {},
   "source": [
    "# Read in Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14fb0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_data = r'Datasets\\cifar10_{}angles_{}dB_train_data.pkl'.format(angles, noise)  # Path to training dataset\n",
    "train_data = pd.read_pickle(path_to_train_data)  # Read in training dataset\n",
    "size = len(train_data)\n",
    "if UseValidationData:\n",
    "    val_data = train_data.iloc[num_training_samples:min(size, num_training_samples+1000)]\n",
    "    size = len(val_data)\n",
    "    shuffled_data = val_data.sample(frac = 1)\n",
    "train_data = train_data.iloc[:num_training_samples]  # Use only 'num_training_samples' from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973761d3",
   "metadata": {},
   "source": [
    "# Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba732e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Rerunning this cell uses the same model variables\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "val_loss_results = []\n",
    "training_time = []\n",
    "results = []\n",
    "\n",
    "train_data_size, val_data_size = len(train_data), len(val_data)\n",
    "num_batches, num_val_batches = int(np.ceil(train_data_size/batch_size)), int(np.ceil(val_data_size/TestBatchSize))\n",
    "print('Number of updates per epoch = ', num_batches)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    val_loss_avg = tf.keras.metrics.Mean()\n",
    "    shuffled_data = train_data.sample(frac = 1)\n",
    "    start = time.time()\n",
    "    for batch in range(num_batches):\n",
    "        y = scale*np.array(shuffled_data.iloc[batch*batch_size:min([(batch+1)*batch_size,train_data_size])])  # Batch of train data\n",
    "        c_act, y = y[:,np.shape(B.Psi)[0]:], y[:,:np.shape(B.Psi)[0]]  # Seperate into labels and measurements\n",
    "        loss_value, grads = grad.call(model, loss_fnc, y, c_act)  # Compute forward and backward pass giving loss and gradients\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))  # Apply network gradients using 'optimizer'\n",
    "        epoch_loss_avg.update_state(loss_value)  # Update average loss for epoch\n",
    "    training_time.append(time.time()-start)  # Saving batch training time\n",
    "    train_loss_results.append(epoch_loss_avg.result())  # Update training loss over all epochs\n",
    "    print(\"Training Time per Epoch: {:.3e} | Total Training Time: {:.3e}\".format(sum(training_time)/(epoch+1), sum(training_time)))\n",
    "    for batch in range(num_val_batches):\n",
    "        y = scale*np.array(val_data.iloc[batch*TestBatchSize:min([(batch+1)*TestBatchSize,val_data_size])])\n",
    "        c_act, y = y[:,np.shape(B.Psi)[0]:], y[:,:np.shape(B.Psi)[0]]\n",
    "        val_loss = loss_fnc.call(c_act, model(y, training=False))\n",
    "        val_loss_avg.update_state(val_loss)\n",
    "    val_loss_results.append(val_loss_avg.result())\n",
    "    results.append([epoch+1, epoch_loss_avg.result(), val_loss_avg.result(), sum(training_time)])\n",
    "    print(\"Epoch {:02d}: Model Loss: {:.3e} | Test Loss: {:.3e}\".format(epoch+1, epoch_loss_avg.result(), val_loss_avg.result()))\n",
    "    if (epoch+1)%1==0:  # How often to save network weights and training results\n",
    "        r = pd.DataFrame(np.array(results))\n",
    "        r.to_csv(r'{}\\training_loss_results.csv'.format(path_to_save)) \n",
    "        model.save_weights(r'{}\\weights_epoch{}.h5'.format(path_to_save, epoch+1))  # Save network weights\n",
    "        \n",
    "model.save_weights(r'{}\\weights.h5'.format(path_to_save))  # Save network weights\n",
    "r = pd.DataFrame(np.array(results))\n",
    "r.to_csv(r'{}\\training_loss_results.csv'.format(path_to_save))  # Save training results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oldtf",
   "language": "python",
   "name": "oldtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
